{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/ipython\n",
    "\n",
    "# ---LICENSE-BEGIN - DO NOT CHANGE OR MOVE THIS HEADER\n",
    "# This file is part of the Neurorobotics Platform software\n",
    "# Copyright (C) 2014,2015,2016,2017 Human Brain Project\n",
    "#\n",
    "# This program is free software; you can redistribute it and/or\n",
    "# modify it under the terms of the GNU General Public License\n",
    "# as published by the Free Software Foundation; either version 2\n",
    "# of the License, or (at your option) any later version.\n",
    "#\n",
    "# This program is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with this program; if not, write to the Free Software\n",
    "# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.\n",
    "# ---LICENSE-END\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append('/home/xilinx/nest/lib/python3.6/site-packages')\n",
    "import pyNN.nest as sim\n",
    "import pathlib as plb\n",
    "import time\n",
    "import pickle\n",
    "import argparse as ap\n",
    "import network as nw\n",
    "import visualization as vis\n",
    "from pyNN.utility.plotting import Figure, Panel\n",
    "# parser = ap.ArgumentParser('./dump-c1-spikes.py --')\n",
    "# parser.add_argument('--dataset-label', type=str, required=True,\n",
    "#                     help='The name of the dataset which was used for\\\n",
    "#                     training')\n",
    "# parser.add_argument('--training-dir', type=str, required=True,\n",
    "#                     help='The directory with the training images')\n",
    "# parser.add_argument('--refrac-c1', type=float, default=0.01, metavar='0.01',\n",
    "#                     help='The refractory period of neurons in the C1 layer in\\\n",
    "#                     ms')\n",
    "# parser.add_argument('--sim-time', default=50, type=float, help='Simulation time',\n",
    "#                     metavar='50')\n",
    "# parser.add_argument('--scales', default=[1.0, 0.71, 0.5, 0.35],\n",
    "#                     nargs='+', type=float,\n",
    "#                     help='A list of image scales for which to create\\\n",
    "#                     layers. Defaults to [1, 0.71, 0.5, 0.35]')\n",
    "# parser.add_argument('--threads', default=1, type=int)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "training_dir ='/home/xilinx/jupyter_notebooks/snn_object_recognition/train_image'\n",
    "dataset_label='/home/xilinx/jupyter_notebooks/snn_object_recognition/train.txt'\n",
    "threads_num=1\n",
    "scales_ = [1.0, 0.71, 0.5, 0.35]\n",
    "refrac_c1=0.01\n",
    "sim_time=50\n",
    "\n",
    "training_path = plb.Path(training_dir)\n",
    "\n",
    "imgs = [(filename.stem, cv2.imread(filename.as_posix(), cv2.CV_8UC1))\\\n",
    "            for filename in sorted(training_path.glob('*'+'.jpg'))]\n",
    "\n",
    "sim.setup(threads=threads_num)\n",
    "\n",
    "layer_collection = {}\n",
    "\n",
    "print('Create S1 layers')\n",
    "t1 = time.time()\n",
    "layer_collection['S1'] =\\\n",
    "    nw.create_gabor_input_layers_for_scales(imgs[0][1], scales_)\n",
    "print('S1 layer creation gabor_input_layers took {} s'.format(time.time() - t1))\n",
    "t2 = time.time()\n",
    "nw.create_cross_layer_inhibition(layer_collection['S1'])\n",
    "print('S1 layer creation layer_inhibition took {} s'.format(time.time() - t2))\n",
    "\n",
    "\n",
    "print('Create C1 layers')\n",
    "t1 = time.time()\n",
    "layer_collection['C1'] = nw.create_C1_layers(layer_collection['S1'],\n",
    "                                             refrac_c1)\n",
    "\n",
    "print(\"Projection.get('weight')::\",layer_collection['C1'].get('weight'))\n",
    "print('C1 creation took {} s'.format(time.time() - t1))\n",
    "t2 = time.time()\n",
    "nw.create_local_inhibition(layer_collection['C1'])\n",
    "print('C1 creation create_local_inhibition took {} s'.format(time.time() - t2))\n",
    "\n",
    "for layer_name in ['C1']:\n",
    "    if layer_name in layer_collection:\n",
    "        for layers in layer_collection[layer_name].values():\n",
    "            for layer in layers:\n",
    "                layer.population.record('spikes')\n",
    "             \n",
    "# Simulate for a certain time to allow the whole layer pipeline to \"get filled\"\n",
    "#sim.run(2)\n",
    "dataset_label = '{}_{}imgs_{}ms_{}px_scales'.format(dataset_label,\n",
    "                                   len(imgs), sim_time,\n",
    "                                   imgs[0][1].shape[0])\n",
    "\n",
    "c1_plots_dir_path = plb.Path('plots/C1/' + dataset_label)\n",
    "if not c1_plots_dir_path.exists():\n",
    "        c1_plots_dir_path.mkdir(parents=True)\n",
    "\n",
    "\n",
    "print('========= Start simulation =========')\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "for filename, target_img in imgs:\n",
    "    \n",
    "    print('Simulating for', filename, 'number', count)\n",
    "    count += 1\n",
    "    t2=time.time()\n",
    "    nw.set_i_offsets_for_all_scales_to(layer_collection['S1'], target_img)\n",
    "    print('set_i_offsets_for_all_scales_to', time.time() - t2, 'seconds')\n",
    "    t1 = time.time()\n",
    "    sim.run(sim_time)\n",
    "\n",
    "    #vis.plot_C1_spikes(layer_collection['C1'],\n",
    "    #                       '{}_image_{}'.format(dataset_label, count),\n",
    "    #                       out_dir_name=c1_plots_dir_path.as_posix())\n",
    "    #sim.run(1)\n",
    "    print('Took', time.time() - t1, 'seconds')\n",
    "end_time = time.time()\n",
    "print('========= Stop  simulation =========')\n",
    "print('Simulation took', end_time - start_time, 's')\n",
    "\n",
    "ddict = {}\n",
    "\n",
    "i=0\n",
    "for size, layers in layer_collection['C1'].items():\n",
    "    ddict[size] = [{'segment': layer.population.get_data().segments[0],\n",
    "                    'shape': layer.shape,\n",
    "                    'label': layer.population.label } for layer in layers]\n",
    "    for layer in layers:\n",
    "      f=open('result/result_'+str(size)+str(layer.population.label)+'.txt','w+')\n",
    "      f.write(str(layer.population.get_data().segments[0].spiketrains))\n",
    "      f.close()\n",
    "\"\"\"\n",
    "    for layer in layers:\n",
    "        data = layer.population.get_data().segments[0]\n",
    "        print(data)\n",
    "        #vm = data.filter(name=\"v\")[0]\n",
    "        #spike = data.filter(name=\"spike\")[0]\n",
    "        i=i+1\n",
    "        Figure(\n",
    "          Panel(data.spiketrains, xlabel=\"Time (ms)\", xticks=True)\n",
    "          ).save(\"simulation_results\"+str(i)+\".png\")\n",
    "    dataset_label += '_{}'.format(size)\n",
    "\"\"\"\n",
    "    \n",
    "#dumpname = 'C1_spikes/{}.txt'.format(dataset_label)\n",
    "#print('Dumping spikes for all scales and layers to file', dumpname)\n",
    "#dumpfile = open(dumpname, 'wb')\n",
    "#pickle.dump(ddict, dumpfile, protocol=4)\n",
    "#dumpfile.close()\n",
    "\n",
    "sim.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
